{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy import *\n",
    "from IPython.html.widgets import *\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation Exercise\n",
    "\n",
    "In this exercise, we will implement the backpropagation algorithm to train a simple neural network with a single hidden layer.\n",
    "\n",
    "<img src=\"files/images/Backpropagation/diagram.png\" style=\"margin:auto; width: 330px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is backpropagation?\n",
    "\n",
    "Previously, we have derived the stochastic gradient decent rule for a single-layer network (perceptron).  (Error) backpropagation algorithm is a learning rule that lets us train feed-forward networks with more layers. \n",
    "\n",
    "## Notation\n",
    "\n",
    " * Input nodes: $ {\\bf x} = (x_1, x_2, ... x_i)$\n",
    " * Hidden nodes: $ {\\bf z} = (z_1, z_2, ... z_j)$\n",
    " * Output nodes: $ {\\bf y} = (y_1, y_2, ... y_k)$\n",
    "\n",
    "The input nodes are connected to the hidden nodes via weights $w^{(h)}_{ij}$, and hidden nodes to output nodes via $w^{(y)}_{jk}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Forward propagation\n",
    "\n",
    "Given a particular input ${\\bf x}$, we calculate the value (or **activation**) of a particular hidden node $z_j$ by taking the weighted sum of the inputs with $w^{(h)}_{ij}$ and passing it through an **activation function** $\\sigma$:\n",
    "\n",
    "$$ z_j = \\sigma \\left( \\sum_i x_i w^{(h)}_{ij} \\right)$$\n",
    "\n",
    "The activation function is a *nonlinear* function.  A popular choice is a sigmoid family, for instance what's called the \"funny tanh\" function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB5zII+6oV/aBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11305f310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def funny_tanh(x):\n",
    "    return 1.7159 * tanh(x*2/3)\n",
    "\n",
    "xs=linspace(-3,3,100)\n",
    "plt.plot(xs,funny_tanh(xs)); plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We use the \"funny tanh\" function, because (a) $f(\\pm 1) = \\pm 1$, (b) the second derivative is a maximum at $x=1$, and (c) the effective gain (change in the variance) is close to 1.  (LeCun, 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.std() is 0.999722\n",
      "tanh(X).std() is 0.628591\n",
      "funny_tanh(X).std() is 0.866664\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import standard_normal\n",
    "X=standard_normal(10000)\n",
    "print(\"X.std() is %f\" % X.std()) # should be close to 1\n",
    "print(\"tanh(X).std() is %f\" % tanh(X).std())\n",
    "print(\"funny_tanh(X).std() is %f\" % funny_tanh(X).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can calculate all values of the hidden nodes at once using the vector notation:\n",
    "\n",
    "$$ {\\bf z} = \\sigma( {\\bf x} \\cdot W^{(h)} ) $$\n",
    "\n",
    "And, similarly, the values of the output nodes are calculated as\n",
    "\n",
    "$$ {\\bf y} = \\sigma( {\\bf z} \\cdot W^{(o)} ) $$\n",
    "\n",
    "You can do the vector-matrix multiplication in `numpy` using the `dot` function, e.g. `dot(x, Wh)`.\n",
    "\n",
    "This is called the **forward propagation** phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Exercise 1. Implement the feed-forward calculation.  Use `tanh` as the activation function $\\sigma()$.\n",
    "\n",
    "$$ {\\bf z} = \\sigma( {\\bf x} \\cdot W^{(h)} ) \\\\ {\\bf y} = \\sigma( {\\bf z} \\cdot W^{(o)} )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def with_bias(x):\n",
    "    return c_[x, ones((x.shape[0], 1))]\n",
    "\n",
    "def feed_forward(x, Wh, Wo, activation_function=funny_tanh):\n",
    "    \"\"\"Calculates the activations for the hidden and output nodes.\n",
    "    \n",
    "    inputs:\n",
    "        x: N x I matrix, where each row is a particular observation.\n",
    "        Wh: I x J matrix of hidden weights.\n",
    "        Wo: J x K matrix of output weights.\n",
    "        activation_function: the activation function to use.\n",
    "    \n",
    "    returns:\n",
    "        z: N x J matrix of hidden activations.\n",
    "        y: N x K matrix of output activations.\n",
    "    \"\"\"\n",
    "    z = activation_function(dot(with_bias(x), Wh))\n",
    "    y = activation_function(dot(with_bias(z), Wo))\n",
    "    return z, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Backward propagation\n",
    "\n",
    "We then calculate \"errors\" associated with each node in the network.  This is called the **backward propagation** phase.\n",
    " \n",
    "As was in the perceptron, the goal of the training is to change the weights $W^{(h)}, W^{(o)}$ so that the current output ${\\bf y}$ will be closer to the target values ${\\bf t}$.  For the squared error loss, we have\n",
    "\n",
    "$$ E = \\frac{1}{2} ({\\bf t} - {\\bf y})^\\intercal ({\\bf t} - {\\bf y}) $$\n",
    "\n",
    "We consider the \"errors\" associated with each unit, which is the negative of the partial derivative of the error with respect to the activations: [TODO: these aren't exactly matching what's in the code, need to be FIXED]\n",
    "\n",
    " * For the output unit $y_k$, we define the \"error\" as $\\delta^{(o)}_k = t_k - y_k$.\n",
    " * For the hidden unit $z_j$, we define the \"error\" as $\\delta^{(h)}_j = \\sigma' ({\\bf x \\cdot W^{(h)}}) \\sum_k w^{(o)}_{jk} \\delta^{(o)}_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It looks a bit hairy, but it's conceptually simple:\n",
    "\n",
    " * For the output units, the error is positive (and proportional to) how much my prediction $y_k$ overshot the target $t_k$.\n",
    " * For the hidden units, the error is a *sum* of all the output unit errors it's connected to (*not* the input units).  \n",
    " \n",
    "The delta of the hidden units are also multiplied by the derivative of the activation function (i.e. the slope of the activation function), because the \"errors\" (the $\\delta$s) are actually partial derivatives of the loss function with respect to the weights.\n",
    "\n",
    "<img src=\"files/images/Backpropagation/diagram2.png\" style=\"margin:auto; width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Exercise 2: implement the routine to calculate the errors (deltas).  [TODO: fix these]\n",
    "\n",
    "$$\n",
    "    \\delta^{(o)} = \\underbrace{y - t}_{N \\times K} \\\\\n",
    "    \\delta^{(h)} = \\underbrace{\\sigma' ({\\bf x \\cdot W^{(h)}})}_{N \\times J} \\times \\underbrace{(\\delta^{(o)} \\cdot W^{(o)\\intercal})}_{N \\times J}\n",
    "$$\n",
    "\n",
    "Here, I've used the $\\times$ symbol to denote the element-wise multiplication, which can be done by using the `multiply` function in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Sample solution:\n",
    "def approximate_derivative(f, x):\n",
    "    \"\"\"Return the (first-order) approximation of the derivative of f at x.\"\"\"\n",
    "    epsilon=1e-8\n",
    "    return (f(x+epsilon) - f(x)) / epsilon\n",
    "\n",
    "def calculate_deltas(error, z, x, Wh, Wo, activation_function=funny_tanh):\n",
    "    \"\"\"Calculates the delta values for the output and hidden nodes.\n",
    "    \n",
    "    inputs:\n",
    "        error: N x K matrix of errors.\n",
    "        z: N x J matrix of hidden activations.\n",
    "        x: N x I matrix, where each row is a particular observation.\n",
    "        Wh: I x J matrix of hidden weights.\n",
    "        Wo: J x K matrix of output weights.\n",
    "        activation_function: the activation function to use.\n",
    "    \n",
    "    returns:\n",
    "        dh: N x J matrix of deltas for the hidden nodes.\n",
    "        do: N x K matrix of deltas for the output nodes.\n",
    "    \"\"\"\n",
    "    do = multiply(approximate_derivative(activation_function, y), error)\n",
    "    # TODO fix the treatment of the bias weight\n",
    "    dh = multiply(approximate_derivative(activation_function, z), dot(do, Wo[:-1].T))\n",
    "    return dh, do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Weight update phase\n",
    "\n",
    "After calculating all $\\delta_k$ and $\\delta_j$s, we update the weights using the stochastic gradient rule:\n",
    "\n",
    "$$\n",
    " w^{(o)}_{jk} \\leftarrow w^{(o)}_{jk} + \\eta z_j \\delta^{(o)}_k \\\\\n",
    " w^{(h)}_{ij} \\leftarrow w^{(h)}_{ij} + \\eta x_i \\delta^{(h)}_j \\\\\n",
    "$$\n",
    "\n",
    "Note that the amount the weight changes is also modulated by the activation of the node: if a node had a large activation, weights associated with it is changed by a large amount, too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Exercise 3. Implement the routine to update the weights.\n",
    "\n",
    "We can do the updates one sample observation $x_i$ at a time (online learning), or just use the average of the delta values (batch learning). For the batch learning, the update rule would be:\n",
    "\n",
    "$$\n",
    " \\underbrace{W^{(o)}}_{J \\times K} \\leftarrow W^{(o)} +  \\frac{\\eta}{N} \\cdot \\underbrace{{\\bf z}^\\intercal}_{K \\times N} \\cdot \\underbrace{\\delta^{(o)}}_{N \\times K}  \\\\\n",
    " \\underbrace{W^{(h)}}_{I \\times J} \\leftarrow W^{(h)} + \\frac{\\eta}{N} \\cdot \\underbrace{{\\bf x}^\\intercal}_{I \\times N} \\cdot \\underbrace{\\delta^{(h)}}_{N \\times J} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def update_weights(Wh, Wo, z, do, x, dh, eta = 0.1):\n",
    "    \"\"\"Updates the hidden and output weights and return them.\n",
    "    \n",
    "    inputs:\n",
    "        Wh: I x J matrix of hidden weights.\n",
    "        Wo: J x K matrix of output weights.\n",
    "        z: N x J matrix of hidden activations.\n",
    "        do: N x K matrix of deltas for the output nodes.\n",
    "        x: N x I matrix, where each row is a particular observation.\n",
    "        dh: N x J matrix of deltas for the hidden nodes.\n",
    "        eta: the learning rate to use.\n",
    "    \n",
    "    returns:\n",
    "        updated Wh and Wo.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    Wo = Wo + eta / N * dot(with_bias(z).T, do)\n",
    "    Wh = Wh + eta / N * dot(with_bias(x).T, dh)\n",
    "    return Wh, Wo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting them all together\n",
    "\n",
    "Now we just need to put them together and update the weight matrices until we get the desired performance out of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bp_update_weights(Wh, Wo, x, t, activation_function=funny_tanh, eta=0.1):\n",
    "    \"\"\"Updates Wh, Wo and returns them from the traning set x and targets t.\"\"\"\n",
    "    z, y = feed_forward(x, Wh, Wo, activation_function=activation_function)\n",
    "    dh, do = calculate_deltas(t-y, z, x, Wh, Wo, activation_function=activation_function)\n",
    "    Wh, Wo = update_weights(Wh, Wo, z, do, x, dh, eta = eta)\n",
    "    return Wh, z, Wo, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Function to plot the network\n",
    "def plot_nn(x, Wh, z, Wo, y):\n",
    "    fig, ax = plt.subplots() \n",
    "    cmap = plt.get_cmap('RdBu')\n",
    "    for i, x_ in enumerate(x.T):\n",
    "        ax.add_artist(plt.Circle((1, -i-1), 0.25, ec='k', fc=cmap(x_[0]/2+.5)))\n",
    "    \n",
    "    for i, wi in enumerate(Wh):\n",
    "        for j, w_ in enumerate(wi):\n",
    "            ax.add_artist(plt.arrow(1.3, -i-1, 1.3, (i-j)*.9, head_width=0.05, head_length=0.1, ec=cmap(w_/2+.5)))\n",
    "\n",
    "    for i, z_ in enumerate(z.T):\n",
    "        ax.add_artist(plt.Circle((3, -i-1), 0.25, ec='k', fc=cmap(z_[0]/2+.5)))\n",
    "        \n",
    "    for i, wi in enumerate(Wo):\n",
    "        for j, w_ in enumerate(wi):\n",
    "            ax.add_artist(plt.arrow(3.3, -i-1, 1.3, (i-j)*.9, head_width=0.05, head_length=0.1, ec=cmap(w_/2+.5)))\n",
    "        \n",
    "    for i, y_ in enumerate(y.T):\n",
    "        ax.add_artist(plt.Circle((5, -i-1), 0.25, ec='k', fc=cmap(y_[0]/2+.5)))\n",
    "        \n",
    "    m = max(x.shape + z.shape + y.shape)\n",
    "    ax.set_xlim([0, 6])\n",
    "    ax.set_ylim([-m-1,0])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "plot_nn(array([[1.0,-1,1]]), -ones((3,3)), ones((1,3)), ones((3, 2)), zeros((1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try training it to recognize the \"XOR\" pattern, and watch how it modifies the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=array([\n",
    "    [ 1,-1],\n",
    "    [-1, 1],\n",
    "    [ 1, 1],\n",
    "    [-1,-1]], dtype=float)\n",
    "T=array([\n",
    "    [ 1],\n",
    "    [ 1],\n",
    "    [-1],\n",
    "    [-1]], dtype=float)\n",
    "\n",
    "# Initialize weights\n",
    "J = 2 # Number of hidden units\n",
    "from numpy.random import random_sample\n",
    "Wh = random_sample((X.shape[1] + 1, J))\n",
    "Wo = random_sample((J + 1, T.shape[1]))\n",
    "epoch = 0\n",
    "\n",
    "def update_and_plot():\n",
    "    global X, T, epoch, Wh, Wo\n",
    "    # Train one example at a time (online learning)\n",
    "    x = X[epoch % X.shape[0],newaxis]\n",
    "    t = T[epoch % T.shape[0],newaxis]\n",
    "    print('x=' + str(x))\n",
    "    print('t=' + str(t))\n",
    "    Wh, z, Wo, y = bp_update_weights(Wh, Wo, x, t, eta=0.9) \n",
    "    print('y=' + str(y))\n",
    "    plot_nn(x, Wh, z, Wo, y)\n",
    "    epoch += 1\n",
    "\n",
    "interact_manual(update_and_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train for some number of epochs and plot the errors.\n",
    "J = 2 # Number of hidden units\n",
    "from numpy.random import random_sample\n",
    "Wh = random_sample((X.shape[1] , J))\n",
    "Wo = random_sample((J + 1, T.shape[1]))\n",
    "\n",
    "epochs=1000\n",
    "rmse=zeros(epochs)\n",
    "cerr=zeros(epochs)\n",
    "for epoch in range(epochs):\n",
    "    Wh, z, Wo, y = bp_update_weights(Wh, Wo, X, T)\n",
    "    z, y = feed_forward(X, Wh, Wo)\n",
    "    errors = T - y\n",
    "    rmse[epoch] = sqrt((errors**2).mean())  # RMSE\n",
    "    cerr[epoch] = (sign(T) != sign(y)).mean()\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].plot(rmse)\n",
    "axs[1].plot(cerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will then test it on the face data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset = pickle.load(open('data/cafe.pkl','r'))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def train(X, t, epochs = 100, n_input = 10, n_hidden = 40, \n",
    "        hidden_learning_rate = 0.1, output_learning_rate = 0.1, momentum_learning_rate = 0.9,          \n",
    "        test_size=0.2, cv=3):\n",
    "    \"\"\"Initialize the network and start training.\"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    n_output = len(unique(t))\n",
    "    Vh = zeros((n_input + 1, n_hidden))\n",
    "    Wh = random_sample(Vh.shape)\n",
    "    Vo = zeros((n_hidden + 1, n_output))\n",
    "    Wo = random_sample(Vo.shape)\n",
    "\n",
    "    # Convert to binary\n",
    "    tb = ones((X.shape[0], n_output))*(-1.0)\n",
    "    tb[arange(tb.shape[0]), t-1]=1.0\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, tb_train, tb_test = train_test_split(X, tb, test_size=test_size, random_state=0)\n",
    "\n",
    "    # Preprocess the data using PCA\n",
    "    pca = PCA(n_components = n_input, whiten=True, copy=True)\n",
    "    Xw_train = pca.fit_transform(X_train)\n",
    "    Xw_test = pca.transform(X_test)\n",
    "\n",
    "    # Start the training\n",
    "    rmse=zeros((epochs,2))\n",
    "    cerr=zeros((epochs,2))\n",
    "    for epoch in arange(epochs):\n",
    "\n",
    "        # Test then Train, since we'll use the training errors\n",
    "        for i, (Xw, tb_) in enumerate([[Xw_test, tb_test], [Xw_train, tb_train]]):\n",
    "            z, y = feed_forward(Xw, Wh, Wo)\n",
    "            errors = tb_ - y\n",
    "            rmse[epoch, i] = sqrt((errors**2).mean())  # RMSE\n",
    "            cerr[epoch, i] = (argmax(tb_,axis=1) != argmax(y,axis=1)).mean()\n",
    "\n",
    "        yield rmse, cerr, epoch, epochs\n",
    "\n",
    "        # Update weights using backpropagation\n",
    "        Wh, z, Wo, y = bp_update_weights(Wh, Wo, Xw_train, tb_train)  # Note they come from the training data!\n",
    "\n",
    "def plot_training(axs, rmse, cerr, t, epochs):\n",
    "    \"\"\"Draw the plot to the specified axis.\"\"\"\n",
    "    axs[0].set_title(\"RMSE\")\n",
    "    axs[0].set_xlabel(\"Training epoch\")\n",
    "    axs[0].set_ylabel(\"RMSE\")\n",
    "    axs[0].grid()\n",
    "\n",
    "    axs[0].plot(arange(t), rmse[:t])\n",
    "    axs[0].set_xlim([0, epochs])\n",
    "    axs[0].set_ylim([0, 2.0])\n",
    "    axs[0].legend(['Test', 'Training'], loc=\"best\")\n",
    "\n",
    "    axs[1].set_title(\"Classification Error\")\n",
    "    axs[1].set_xlabel(\"Training epoch\")\n",
    "    axs[1].set_ylabel(\"Classification Error Rate [\\%]\")\n",
    "    axs[1].grid()\n",
    "\n",
    "    axs[1].plot(arange(t), cerr[:t]*100.0)\n",
    "    axs[1].set_xlim([0, epochs])\n",
    "    axs[1].set_ylim([0, 100.0])\n",
    "    axs[1].legend(['Test', 'Training'], loc=\"best\")\n",
    "\n",
    "def train_network(dataset, target_name, **kwargs):\n",
    "    X = dataset.data\n",
    "    t = dataset[target_name]\n",
    "    fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "    for rmse, cerr, epoch, epochs in train(X, t, **kwargs):\n",
    "        if mod(epoch, 10) != 0:\n",
    "            continue\n",
    "\n",
    "        plot_training(axs, rmse, cerr, epoch, epochs)\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "        axs[0].cla()\n",
    "        axs[1].cla()\n",
    "\n",
    "    plt.close()\n",
    "    plot_training(axs, rmse, cerr, epoch, epochs)\n",
    "    clear_output(wait=True)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interact_manual(train_network,\n",
    "    dataset=fixed(dataset),\n",
    "    target_name={'Identity': 'target', 'Gender': 'gender', 'Expression': 'expression'},\n",
    "    epochs= IntSlider(min=10, max=500, step=10, value=100),\n",
    "    n_input = IntSlider(min=1, max=(dataset.data.shape[0]-1), value=40),\n",
    "    n_hidden = IntSlider(min=1, max=40, value=10), \n",
    "    hidden_learning_rate = FloatSlider(min=0.01, max=0.3, value=0.1),\n",
    "    output_learning_rate = FloatSlider(min=0.01, max=0.3, value=0.1),\n",
    "    momentum_learning_rate = FloatSlider(min=0.5, max=0.99, value=0.9),         \n",
    "    test_size=FloatSlider(min=0.1, max=0.5, value=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
